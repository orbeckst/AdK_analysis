# $Id: angles.py 4108 2010-04-27 14:20:09Z root $
"""Database of frames stored with
  LID angles
  NMP angle
  frame number
  trajectory file
  .... (observables)

Source:

  angle/*pmf*.pickle  (produced by angles_adk.py)


  See AdK.analysis.

* Add data required for WHAM: for each trajectory we need to have

  force constant
  reference value for NMP and LID angle

* Force constants: see pmf/cons_value.txt

* Reference angles: pmf/*.txt

  The trajectory name portion is encoded in the filename; the file
  itself contains the numbering of the simulation and the reference
  value:

     (?P<trjname>.*)\.txt:  windowID   LID_0  NMP_0  --> <trjname>_<windowID>

     - windowID: is treated as an integer: 001 --> 1, 003 --> 3, ...
       (a few pickle files were MANUALLY renamed to correspond to this
       scheme; typically the ones that were started from specific
       frames of a trajectory such as co127_044, co596_078,
       oc100_0093642)
     - The full name of the corresponding pickle file should be 

         <trjname>_int(<windowID>)_pmf_angles.pickle
    
     - Nevertheless, we also match 003 etc in the filename for
       <window_ID>, ie any number of leading zeros.

* autoumbrella jobs: look for the windows.autometa file, which lists
  all data per trajectory; these jobs should not be listed in txt
  files or the cons_values.txt file.

  See PMF.umbrella.Autometa for details.

  
* energy:

  Input files generated by make_energy.py; see AdK.analysis


  

Notes on the database schema
============================

Main table of the AngleDB (typically accessed as __self__):

  angles         TID, filename, trajectory, frame, LID, NMP, energy

TID is a foreign key into __meta__. If it is NULL then the data are
not to be considered for analysis (there is no entry in __meta__). The
__data__ view shows TID NOTNULL?.

Auxiliary table (__meta__):

  trajectories   TID PRIMARY KEY, filename, forceconstant, LID_ref, NMP_ref, dcdpath UNIQUE           

     
"""

import cPickle
import os
import re
import glob
import bz2
import warnings
try:
    # pysqlite2 was the one one used for development
    from pysqlite2 import dbapi2 as sqlite
except ImportError:
    # newer versions
    import sqlite3 as sqlite
import numpy
import math

# PMF.config is setup with bin/install.sh
# (and generated from lib/templates/config.py.m4)
import config

class BaseSelection(object):
    """Base class for SQL-based tables: functions that operate on results of SELECT.

    The actual database class should be derived from BaseSelection but note:
    * override __init__, only use selfnames, eg super(AngleDB,self).__init__(selfname="angles")
    * override SQL(): the db class MUST provide the real implementation and not just the wrapper
    * avoid setting db <-- self to avoid a cyclic reference (?)
    """

    def __init__(self,*args,**kwargs):
        # just as a reminder, still a bit bumpy (AngleDB ignores most of it and __init__s itself mostly)
        self.selfname = kwargs.setdefault('selfname',None)  # name of main table/view
        self.metaname = kwargs.setdefault('metaname',None)  # name of aux trajectories table
        self.dataname = kwargs.setdefault('dataname',None)  # view of __self__ w/o blacklisted trj 
        self.__numrows = None                               # size of db/selection: set in init or dynamically
        self.db = kwargs.setdefault('db',None)              # database
        if self.db is not None:
            self.metaname = (self.metaname or self.db.metaname) # hack to get __meta__ table in
            if not hasattr(self,'connection'):              # AngleDB(BaseSelection) already has it
                self.connection = self.db.connection
            self.cursor = self.connection.cursor()

        # SQL function replaces each key with the value of the attribute; can be used to make the SQL more
        # readable; this list can be modified later externally when other tables are added
        # (This must be an attribute lookup so that __init__ of child classes can set these values appropriately;
        # the convention is to call the attribute XXXname and the place holder __XXX__.)
        self.SQL_transformation_rules = {'__self__': 'selfname',
                                         '__meta__': 'metaname',
                                         '__data__': 'dataname',  # see _add_data_VIEW
                                         '__whamfiles__': None,   # see PMF.wham.Project
                                         }

    def _add_data_VIEW(self,temporary=True):
        """Add a __data__ view (__self__ table which corresponds to what is in __meta__.

        The __data__ view can be used like a table. It will only
        provide those data that have an entry in __meta__, i.e. it
        will always respect the blacklist exclusions.

        It may be slower than __self__ but this has not been benchmarked.
        """
        # Selection()s create TEMP __data__ that shadow the TEMP __self__.
        # XXX AngleDB() _also_ creates a TEMP __data__.

        # XXX: is the VIEW fully created in memory? If yes then that is BAD because
        #      it completely kills the system. Has the TEMP attribute something to do
        #      with this?
        #   -- Currently using TEMP for _all_ views (even the AngleDB one) and use
        #      PRAGMA temp_store = FILE;
        #   -- did not help, back to MEMORY
        
        # __data__ is handled like __self__ (eg otherdataname etc...)
        
        # drop it first to have a clean start
        try:
            self.cursor.execute(self._transform_SQL("DROP VIEW __data__"))
        except sqlite.OperationalError:
            pass
        tempview = ""
        if temporary:
            tempview = "TEMPORARY"
        _SQL = """CREATE %(tempview)s VIEW __data__ AS
                         SELECT * FROM __self__ WHERE TID NOTNULL""" % vars()
        SQL = self._transform_SQL(_SQL)
        try:
            self.cursor.execute(SQL)
        except sqlite.OperationalError, err:
            if str(err).find('no such column: TID') > -1:
                # no need for __data__, use __self__
                # hack the two entry points to __data__ (both are needed,otherwise code breaks)
                self.SQL_transformation_rules['__data__'] = 'selfname'  # needed for general lookup
                self.dataname = self.selfname                           # needed for selection() to work
            else:
                raise
        

    def _transform_SQL(self,SQL):
        """Replaces special strings such as __self__ with table or view name; rules are in self.SQL_transformation_rules"""
        _SQL = SQL
        for target,replacement in self.SQL_transformation_rules.items():
            try:
                _SQL = _SQL.replace(target, str(self.__getattribute__(replacement)))
            except (AttributeError,TypeError):
                pass
        return _SQL
    
    def SQL(self,SQL,*sqlvalues,**kwargs):
        """Execute SQL statement and return results.

        SQL(SQL_statement, *values)

        SQL_statement is a legal SQL statement (see
        http://www.sqlite.org/lang.html for the SQL syntax understood by the
        sqlite3 database engine). It can contain the '?' place holder. In this
        case, *values will be interpolated into the SQL string using the
        dbapi.

        Note: __self__ is replaced by the table name.
              __meta__ is replaced by the name of the auxiliary trajectories table.
              __data__ is replaced by a view that only shows that part of __self__
                       that is also listed in __meta__

        :Arguments:
        *values            one or more values that are referenced with '?' in the SQL
        verbose            show sql statement [False]
        asrecarray         return a numpy rec array [False]
        """
        
        # TODO: Make it optionally behave like an iterator on the
        #       connection.execute (use block_iterator()).

        kwargs.setdefault('verbose',False)
        kwargs.setdefault('asrecarray',False)
        
        _SQL = self._transform_SQL(SQL)
        if kwargs['verbose']:
            print "SQL: '%(_SQL)s'" % vars()
        c = self.db.cursor         # database must make cursor available
        c.execute(_SQL,sqlvalues)
        result = c.fetchall()      # yikes---slurps the whole result into memory
        if kwargs['asrecarray']:
            try:
                names = [x[0] for x in c.description]   # first elements are column names
                result = numpy.rec.fromrecords(result,names=names)
            except:
                #++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
                #
                #   CHECK THIS       ---  XXX
                #
                # TODO: THIS IS BAD: in some cases numpy.rec.fromrecords seems
                # to fail (maybe too many records?) and then we silently return
                # a tuple even though the caller is expecting a record array.
                #
                # Can we simply raise an exception instead? Or does this break code??
                # TODO: MAKE IT FAIL (but check in as one change set)
                pass  # keep as tuples if we cannot convert <--- BAD
                #
                #++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
        return result

    def tolist(self,**kwargs):
        """Return the full selection. See SQL for explanation of kwargs."""
        return self.SQL("""SELECT * FROM __self__""",**kwargs)

    def recarray():
        doc = "Full selection as numpy recarray"
        def fget(self):
            return self.tolist(asrecarray=True)
        return locals()
    recarray = property(**recarray())

    # global descriptors of the data ('canned' SELECTs)

    def columns():
        doc = "names of the columns in the SQL table or view"
        def fget(self):
            # maybe cache it?
            # find column names from database
            self.cursor.execute("""SELECT * FROM %(selfname)s WHERE 0""" % vars(self))
            self.__columns = [x[0] for x in self.cursor.description]
            return self.__columns
        return locals()
    columns = property(**columns())

    def numrows():
        # TODO: should be cached for large dbs
        doc = """Size of the database (number of rows)."""
        def fget(self):
            return self.SQL('SELECT COUNT(*) FROM __self__')[0][0]
        return locals()
    numrows = property(**numrows())

    def filenames(self,pattern="*"):
        """Returns a sorted numpy.array of all filenames from which the selection or db was built.

        Arguments:
        pattern        only return filenames that match the glob pattern, eg pattern='*1dvr*'
        """
        if pattern == '*' or pattern is None:     # speed up x2
            return numpy.ravel(
                self.SQL("""SELECT DISTINCT filename FROM __self__ ORDER BY filename ASC"""))
        else:
            return numpy.ravel(
                self.SQL("""SELECT DISTINCT filename FROM __self__
                            WHERE GLOB(?,filename) ORDER BY filename ASC""", pattern))

    def angleranges(self,deltaNMP=0,deltaLID=0,**limits):
        """Returns the min and max values of the NMP and LID angles.

          (minNMP,maxNMP),(minLID,maxLID) =  angleranges(deltaNMP=0, deltaLID=0, minNMP=None,maxNMP=None,minLID=None,maxLID=None)

        If any of the keyword arguments are set they are substituted for the data-derived values.

        Arguments:
        deltaNMP         + subtract this value from data-derived minNMP, add this value to maxNMP
                         | (NOT applied to limits provided as arguments!)
        deltaLID         | same for LID
        minNMP           + substitute this value for the data-derived value
        maxNMP           |
        minLID           | 
        maxLID           |
        """        
        keys = ('minNMP','maxNMP','minLID','maxLID')
        
        data_limits = dict(zip(keys,self.SQL("""SELECT min(NMP)-?,max(NMP)+?,min(LID)-?,max(LID)+? FROM __self__""",
                                             deltaNMP,deltaNMP, deltaLID,deltaLID)[0]))
        for k in keys:
            limits.setdefault(k,data_limits[k])
        return (limits['minNMP'],limits['maxNMP']), (limits['minLID'],limits['maxLID'])        

    # range_of() is generalization of angleranges()
    def range_of(self, variable, delta=0, **limits):
        """Returns the min and max values of the *variable* angles.

        If any of the keyword arguments are set they are substituted for the data-derived values.

        :Arguments:
           variable
              name of the variable (SQL column name)
           delta        
              subtract this value from data-derived min, add this value to max
              NOT applied to limits provided as arguments vmin or vmax
           vmin
              override data-derived min
           vmax
              override data-derived max
        :Returns:  (min Var, max Var)
        """        
        keys = ('vmin', 'vmax')
        _SQL = """SELECT min(%(variable)s)-?,max(%(variable)s)+? FROM __self__""" % vars()
        data_limits = dict(zip(keys,self.SQL(_SQL,delta,delta)[0]))
        for k in keys:
            limits.setdefault(k,data_limits[k])
        return limits['vmin'], limits['vmax']        

    # selections:
    
    def qrange(self,NMP=None,LID=None,asrecarray=False):
        """Find all frames for which the angles are within the given range.

        db.qrange(NMP,LID) --> list of trajectories and frames

        Example:
         db.qrange(NMP=(None,80),LID=(90,95.5))

        Angle ranges are given as tuples; if one boundary is None then it is ignored.
        Boundaries are inclusive, ie LID_min <= angle <= LID_max.
        """
        _operators = (">=", "<=")
        where_clauses = []
        def process_limits(column,limits):
            if limits is not None:
                where_clauses.extend(["%(column)s %(op)s %(val)r" % vars()
                                      for val,op in zip(limits,_operators) if val is not None])
        process_limits('LID',LID)
        process_limits('NMP',NMP)
        where = " AND ".join(where_clauses)

        return self.SQL("""SELECT * FROM __self__ WHERE """+where,
                        verbose=True,asrecarray=asrecarray)

    def qaround(self,NMP=None,LID=None,delta=1.0,asrecarray=False):
        """Find all frames in the range [angle-delta, angle+delta].

        db.qaround(NMP=<angle>,LID=<angle>,delta=1.0) --> list of trajectories and frames
        """
        
        if LID is None and NMP is None:
            warnings.warn("At least one angle should be defined; this returns the whole db.")

        DELTA = numpy.array([-delta,delta])
        LIDrange = NMPrange = None
        if LID:
            LIDrange = LID + DELTA
        if NMP:
            NMPrange = NMP + DELTA
        return self.qrange(LID=LIDrange,NMP=NMPrange,asrecarray=asrecarray)


    #------------------------------------------------------------
    # analysis/plotting
                
    def plot(self,mode='pseudoFE',figname=None,bins_LID=50,cmap=None,title=None,
             max_contour=None,min_contour=None,interval_contour=None,
             global_dimensions=True,clf=True,alpha=1.0,contour_alpha=1.0,
             blocksize=2**16):
        """Plot the distribution of (NMP-angle,LID-angle) as a contour plot.

        plot(**kwargs)

        :Arguments:

        mode                pseudoFE:       -ln p(NMP,LID)
                            density:         n(NMP,LID) = counts/bin_area
                            reldev:          counts/<counts> - 1 (the mean only takes into
                                             account bins with counts > 0)
                                             
        figname             write image to file; suffix determines type, eg png, pdf
        bins_LID            histogram bins in LID dimension; NMP is autoscaled for same delta
        cmap                pylab.cm colourmap instance [hot_r]
        min_contour
        max_contour         cutoff of the contour map in corresponding units
        interval_contour    distance between major contour lines (minors are at 0.5*interval_contour)
        title               title of the plot [None]
        global_dimensions   place it on a canvas which would fit ALL data [True]
        clf                 clear figure canvas and draw legend [True]
        alpha               alpha of plot [1.0]
        contour_alpha       alpha of contour_lines

        blocksize           for large databases the computation must be done in blocks to fit
                            into memory; reduce this number if the process starts swapping
        """
        import pylab

        was_interactive = pylab.matplotlib.is_interactive()
        pylab.matplotlib.interactive(False)

        defaults = {'pseudoFE': {'colorbar': {'extend': 'max'},
                                 'contours': {'max':6.5, 'min':0, 'interval':1},
                                 'cmap': pylab.cm.hot_r,
                                 },
                    'density': {'colorbar': {'extend': 'max'},
                                'contours': {'max':1.0, 'min':0, 'interval':0.1},
                                'cmap': pylab.cm.jet,
                                },
                    'reldev': {'colorbar': {'extend': 'both'},
                               'contours': {'max':2.0, 'min':-1.0, 'interval':0.4},
                               'cmap': pylab.cm.jet,                               
                               },
                    }
        
        if max_contour is None:
            max_contour = defaults[mode]['contours']['max']
        if min_contour is None:
            min_contour = defaults[mode]['contours']['min']
        if interval_contour is None:
            interval_contour = defaults[mode]['contours']['interval']
        
        if cmap is None:
            cmap = defaults[mode]['cmap']

        
        # HACK: global dimensions of plot
        if global_dimensions:
            # relies on presence of self.db to get the global min/max
            try:
                _lim = self.db.angleranges()
            except AttributeError:
                _lim = self.angleranges()
        else:
            _lim = self.angleranges()
        # limits for the plots
        NMPlimits,LIDlimits = _lim
        del _lim

        # limits for the histogram
        # (make it bigger by /-2 degrees in each direction so that it plots more nicely)
        dataNMPlim,dataLIDlim = self.angleranges(deltaNMP=2, deltaLID=2)

        # get full dimensions to calculate bins
        d_LID = numpy.abs(dataLIDlim[1] - dataLIDlim[0])
        d_NMP = numpy.abs(dataNMPlim[1] - dataNMPlim[0])

        ratio = d_NMP/d_LID
        bins_NMP = int(ratio*bins_LID)  # scale bins proportionally in NMP
            
        # Do the histogramming in blocks for big selections because otherwise the memory blows up.
        h = numpy.zeros((bins_LID,bins_NMP),dtype=float)
        h_i = h.copy()
        norm = 0.0
        for all in self.block_iterator(blocksize=blocksize,asrecarray=True):
            ha_LID, ha_NMP = all.LID, all.NMP
            del all                 # release memory (?)
            weight = len(ha_LID)
            norm += weight
            h_i[:],e1,e2 = numpy.histogram2d(ha_LID,ha_NMP,
                                             bins=(bins_LID,bins_NMP),range=[dataLIDlim,dataNMPlim],
                                             normed=True)
            del ha_LID  # release memory immediately
            del ha_NMP  # ... and note that they can be of len < blocksize, so no simple preallocation
            h += weight * h_i
        h /= norm
        del h_i        

        # midpoints of bins
        mLID = 0.5*(e1[1:]+e1[:-1])  # LID
        mNMP = 0.5*(e2[1:]+e2[:-1])  # NMP

        if mode == "pseudoFE":
            # turn density into free energy (could use masked arrays here...)
            F = -numpy.log(h+1e-100)   # add 1e-100 to 'mask' the empty (0) entries
            Fshifted = F - F.min()     # make the endpoints the zero points
        elif mode == "density":
            Fshifted = h
        elif mode == "reldev":
            # only count non-zero bins in the average
            Fshifted = h/h[h>0].mean() - 1
        else:
            raise NotImplementedError("mode = %s is not implemented" % mode)

        if clf:
            pylab.clf()

        # note that plotting the array reverses the axes (we use C-order):
        # array = (row, col) <--> (y, x) !!
        cont = pylab.contourf(mNMP,mLID,Fshifted,numpy.arange(min_contour,max_contour,0.1),
                              extend='max',cmap=cmap,alpha=alpha)
        cont.ax.set_aspect('equal')

        if clf:
            # can't repeatedly add colorbars so we MUST clf first
            pylab.colorbar(extend=defaults[mode]['colorbar']['extend'],
                           ticks=numpy.arange(min_contour,max_contour))

        # light lines at 0.5 x intervals
        pylab.contour(mNMP,mLID,Fshifted,numpy.arange(min_contour,max_contour,interval_contour/2.),
                      colors='k',linewidths=0.2,alpha=0.5*contour_alpha)
        # heavy lines at 1 x intervals
        pylab.contour(mNMP,mLID,Fshifted,numpy.arange(min_contour,max_contour,interval_contour),
                      colors='k',linewidths=0.5,alpha=contour_alpha)

        pylab.xlabel(r'angle NMP-CORE')
        pylab.ylabel(r'angle LID-CORE')
        if title:
            pylab.title(title)

        ax = pylab.gca()
        degreeFormatter = pylab.matplotlib.ticker.FormatStrFormatter(r'%d$^\circ$')
        ax.xaxis.set_major_formatter(degreeFormatter)
        ax.yaxis.set_major_formatter(degreeFormatter)

        pylab.xlim(NMPlimits)
        pylab.ylim(LIDlimits)        

        if was_interactive:
            pylab.draw()
        
        if figname:
            # http://banyan.usc.edu/log/python/plot : use str to work around
            # 'TypeError: cannot return std::string from Unicode object' with Agg
            pylab.savefig(str(figname))

        pylab.matplotlib.interactive(was_interactive)  # revert to previous state


    def plot2d(self,x,y, mode='pseudoFE',figname=None,
               xbins=50,ybins=None,delta=None,cmap=None,title=None,
               max_contour=None,min_contour=None,interval_contour=None,
               global_dimensions=True,clf=True,alpha=1.0,contour_alpha=1.0,
               use_contourf=True,
               blocksize=2**16):
        """Plot the distribution of (x, y) columns as a contour plot.

        plot(**kwargs)

        :Arguments:
        x                   column name
        y                   column name

        mode                pseudoFE:       -ln p(x,y)
                            density:         n(x,y) = counts/bin_area
                            reldev:          counts/<counts> - 1 (the mean only takes into
                                             account bins with counts > 0)
                                             
        figname             write image to file; suffix determines type, eg png, pdf
        xbins               histogram bins in x dimension
        ybins               histogram bins in y dimension; if ``None`` then autoscaled with x
        cmap                pylab.cm colourmap instance [hot_r]
        min_contour
        max_contour         cutoff of the contour map in corresponding units
        interval_contour    distance between major contour lines (minors are at 0.5*interval_contour)
        title               title of the plot [None]
        global_dimensions   place it on a canvas which would fit ALL data [True]
        clf                 clear figure canvas and draw legend [True]
        alpha               alpha of plot [1.0]
        contour_alpha       alpha of contour_lines

        delta               increase size of histograms (``None``: use 5% of the range)
        blocksize           for large databases the computation must be done in blocks to fit
                            into memory; reduce this number if the process starts swapping

        use_contourf         True (false: uses imshow)
        """
        import pylab

        was_interactive = pylab.matplotlib.is_interactive()
        pylab.matplotlib.interactive(False)

        defaults = {'pseudoFE': {'colorbar': {'extend': 'max'},
                                 'contours': {'max':6.5, 'min':0, 'interval':1},
                                 'cmap': pylab.cm.hot_r,
                                 },
                    'density': {'colorbar': {'extend': 'max'},
                                'contours': {'max':1.0, 'min':0, 'interval':0.1},
                                'cmap': pylab.cm.jet,
                                },
                    'reldev': {'colorbar': {'extend': 'both'},
                               'contours': {'max':2.0, 'min':-1.0, 'interval':0.4},
                               'cmap': pylab.cm.jet,                               
                               },
                    }

        # minimum sanity check
        for v in x,y:
            if not v in self.columns:
                raise ValueError("The variable name %r is not a column in the db; only\n%r\nare allowed" %
                                 (v, self.columns))
        
        if max_contour is None:
            max_contour = defaults[mode]['contours']['max']
        if min_contour is None:
            min_contour = defaults[mode]['contours']['min']
        if interval_contour is None:
            interval_contour = defaults[mode]['contours']['interval']
        
        if cmap is None:
            cmap = defaults[mode]['cmap']

        
        # HACK: global dimensions of plot
        if global_dimensions:
            # relies on presence of self.db to get the global min/max
            try:
                _lim = self.db.range_of(x), self.db.range_of(y)
            except (AttributeError, sqlite.OperationalError):
                # OpError: if this a Selection with a column not in db
                _lim = self.range_of(x), self.range_of(y)
        else:
            _lim = self.db.range_of(x), self.db.range_of(y)
        # limits for the plots
        xlimits,ylimits = _lim
        del _lim

        # limits for the histogram
        # (make it bigger by in each direction so that it plots more nicely)
        if delta is None:
            def expandlimits(v, factor=0.025):
                vmin, vmax = self.db.range_of(v)
                delta = factor * numpy.abs(vmax - vmin)
                return vmin-delta, vmax+delta
            dataxlim,dataylim = expandlimits(x), expandlimits(y)
        else:
            dataxlim,dataylim = self.db.range_of(x, delta=delta), self.db.range_of(y, delta=delta)

        # get full dimensions to calculate bins
        d_x = numpy.abs(dataxlim[1] - dataxlim[0])        
        d_y = numpy.abs(dataylim[1] - dataylim[0])

        aspect = 'equal'
        if ybins is None:
            ratio = d_y/d_x
            if numpy.max([ratio, 1/ratio]) > 4:
                # probably not the same kind of data so auto set makes little sense
                ratio = 1
                aspect = "auto"
            ybins = int(xbins * ratio)  # scale bins proportionally in y --- might not always be appropriate

        # alias the selected columns to x and y (from __data__ to avoid empty crap)
        SQL = "SELECT %(x)s AS x, %(y)s AS y FROM __data__ WHERE NOT (x ISNULL OR y ISNULL)" % vars()
        
        # Do the histogramming in blocks for big selections because otherwise the memory blows up.
        h = numpy.zeros((xbins,ybins),dtype=float)
        h_i = h.copy()
        norm = 0.0
        for all in self.block_iterator(blocksize=blocksize, SQL=SQL, asrecarray=True):
            ha_x, ha_y = all.x, all.y  # uses aliases
            del all                    # release memory (?)
            weight = len(ha_x)
            norm += weight
            h_i[:],e1,e2 = numpy.histogram2d(ha_x,ha_y,
                                             bins=(xbins,ybins),range=[dataxlim,dataylim],
                                             normed=True)
            del ha_y  # release memory immediately
            del ha_x  # ... and note that they can be of len < blocksize, so no simple preallocation
            h += weight * h_i
        h /= norm
        del h_i        

        # midpoints of bins
        mx = 0.5*(e1[1:]+e1[:-1])  # x
        my = 0.5*(e2[1:]+e2[:-1])  # y

        if mode == "pseudoFE":
            # turn density into free energy (could use masked arrays here...)
            F = -numpy.log(h+1e-100)   # add 1e-100 to 'mask' the empty (0) entries
            Fshifted = F - F.min()     # make the endpoints the zero points
        elif mode == "density":
            Fshifted = h
        elif mode == "reldev":
            # only count non-zero bins in the average
            Fshifted = h/h[h>0].mean() - 1
        else:
            raise NotImplementedError("mode = %s is not implemented" % mode)

        if clf:
            pylab.clf()

        # note that plotting the array reverses the axes (we use C-order):
        # array = (row, col) <--> (y, x) !!
        # XXX: check mx, my or my,mx
        if use_contourf:
            cont = pylab.contourf(mx,my,Fshifted.T,numpy.arange(min_contour,max_contour,0.1),
                                  extend='max',cmap=cmap,alpha=alpha)
            cont.ax.set_aspect(aspect)
        else:
            # for broken contourf
            extent = (mx.min(), mx.max(), my.min(), my.max())
            pylab.imshow(Fshifted.T, origin='lower', extent=extent, aspect=aspect,
                         vmin=min_contour, vmax=max_contour,
                         cmap=cmap,alpha=alpha)

        if clf:
            # can't repeatedly add colorbars so we MUST clf first
            pylab.colorbar(extend=defaults[mode]['colorbar']['extend'],
                           ticks=numpy.arange(min_contour,max_contour))

        # light lines at 0.5 x intervals
        pylab.contour(mx,my,Fshifted.T,numpy.arange(min_contour,max_contour,interval_contour/2.),
                      colors='k',linewidths=0.2,alpha=0.5*contour_alpha)
        # heavy lines at 1 x intervals
        pylab.contour(mx,my,Fshifted.T,numpy.arange(min_contour,max_contour,interval_contour),
                      colors='k',linewidths=0.5,alpha=contour_alpha)

        # TODO: customize labels
        # (or add column names to Loaders?)
        pylab.xlabel(x)
        pylab.ylabel(y)
        if title:
            pylab.title(title)

        ax = pylab.gca()
        #degreeFormatter = pylab.matplotlib.ticker.FormatStrFormatter(r'%d$^\circ$')
        #ax.xaxis.set_major_formatter(degreeFormatter)
        #ax.yaxis.set_major_formatter(degreeFormatter)

        pylab.xlim(xlimits)
        pylab.ylim(ylimits)        

        if was_interactive:
            pylab.draw()
        
        if figname:
            # http://banyan.usc.edu/log/python/plot : use str to work around
            # 'TypeError: cannot return std::string from Unicode object' with Agg
            pylab.savefig(str(figname))

        pylab.matplotlib.interactive(was_interactive)  # revert to previous state

        # debugging
        self._graph2d = {'x': mx, 'y': my, 'F': Fshifted, 'h': h}

    def plot1d(self, x, y, **kwargs):
        """Plot y over x.

        :Arguments:
          *x*
             abscicssa, column name from db
          *y*
             ordinate, column name from db
          *bins*
             number of bins for the histgramming/averaging [100]
          *global_dimensions*
             use data limits from full db, not just the selection [True]
        """

        import pylab

        # minimum sanity check
        for v in x,y:
            if not v in self.columns:
                raise ValueError("The variable name %r is not a column in the db; only\n%r\nare allowed" %
                                 (v, self.columns))

        bins = kwargs.pop('bins', 100)
        blocksize = kwargs.pop('blocksize', 2**16)

        # HACK: global dimensions of plot
        if kwargs.pop('global_dimensions',True):
            # relies on presence of self.db to get the global min/max
            try:
                _lim = self.db.range_of(x)
            except (AttributeError, sqlite.OperationalError):
                # OpError: if this a Selection with a column not in db
                _lim = self.range_of(x)
        else:
            _lim = self.db.range_of(x)
        # limits for the plots
        xlimits = _lim
        del _lim

        # limits for the histogram
        try:
            dataxlim = self.db.range_of(x)
        except (AttributeError, sqlite.OperationalError):
            # OpError: if this a Selection with a column not in db
            dataxlim = self.range_of(x)

        def histo(a, **histkwargs):
            return numpy.histogram(a, bins=bins, range=dataxlim, new=True, **histkwargs)

        S,edges = histo([0], weights=[1.0])  # force float arrays with weight
        S *= 0                # sum --> average
        S2 = S.copy()         # sum of squares --> stddev
        N, edges = histo([0]) # counts
        N *= 0

        # alias the selected columns to x and y (from __data__ to avoid empty crap)
        SQL = "SELECT %(x)s AS x, %(y)s AS y FROM __data__ WHERE NOT (x ISNULL OR y ISNULL)" % vars()

        # Do the histogramming in blocks for big selections because otherwise the memory blows up.
        h_i = S.copy()

        for all in self.block_iterator(blocksize=blocksize, SQL=SQL, asrecarray=True):
            ha_x, ha_y = all.x, all.y  # uses aliases
            N += histo(ha_x)[0]   # counts

            h_i[:],edges = histo(ha_x, weights=ha_y)
            S += h_i
            h_i[:],edges = histo(ha_x, weights=ha_y**2)
            S2 += h_i

        # finalize
        bins_with_data = (N > 0)                # only use bins that contain data
        S[bins_with_data] /= N[bins_with_data]  # average

        Nmin = 1
        bins_with_data = (N > Nmin)             # N-1 standard deviation
        S2[N <= Nmin] = 0                        # no values for N=0,1 data points (or mask array?)
        S2[bins_with_data] /= N[bins_with_data]
        S2[bins_with_data] -= S[bins_with_data]**2                         # VarX = <X**2> - <X>**2
        S2[bins_with_data] *= N[bins_with_data] / (N[bins_with_data] - 1)  # N/(N-1) * VarX
        S2[bins_with_data] = numpy.sqrt(S2[bins_with_data])                 # final stddev

        mx = 0.5*(edges[1:] + edges[:-1])  # midpoints

        color = kwargs.pop('color', 'black')

        pylab.plot(mx, S, color=color, **kwargs) 
        pylab.errorbar(mx, S, yerr=S2, fmt=None, ecolor=color, capsize=0)

        pylab.xlabel(x)
        pylab.ylabel(y)

        pylab.draw()

        # debugging
        self._graph1d = {'x': mx, 'y': S, 'dy': S2}

        # if using gromacs.formats.XVG
        #x = XVG(output)
        #x.set(numpy.concatenate([[phi, N], S, S2])) ???
        #x.write()

    def plot_xy(self,x,y,trajectory,name="xy",**plotargs):
        """Plot one or more individual trajectories.

          lines = plot_xy(x,y,trajectory,**plotargs)

        *x* and *y* are column names.

        The trajectory argument can be a glob pattern (but right now
        different trajectories are not separated). **plotargs are
        arguments for pylab plot, e.g. ::

            plot_xy('NMP', 'LID', 'oc020', color='red', linewidth2)

        *name* serves as an identifier for the type of lines; e.g. use
        'angles' when plotting trajectories in angle space and
        'contacts' when looking at q1-q2 space.

        Use a database with transitions such as 

           DIMSdb = PMF.angles.setup(':memory:',simulations=['dims'])
        """
        import pylab
        plotargs['scalex'] = plotargs['scaley'] = False
        plotargs.setdefault('color','white')
        plotargs.setdefault('linestyle','-')        

        _SQL = """SELECT %(x)s AS x, %(y)s AS y FROM __self__ 
                  WHERE GLOB(?,trajectory) AND NOT (x ISNULL OR y ISNULL)""" % vars()
        trj = self.SQL(_SQL, trajectory, asrecarray=True)
        if len(trj) == 0:
            raise ValueError('Found no trajectory data')
        if not hasattr(self,'_lines'):
            # extend db object on the fly...
            self._lines = {}   # store graphic objects here
            self._lines_annotation = {}
        if not name in self._lines:
            self._lines[name] = []            # append lines
            self._lines_annotation[name] = [] # append trajectory
        lines = pylab.plot(trj.x, trj.y,'-',**plotargs)
        self._lines[name].extend(lines)
        self._lines_annotation[name].append(trajectory)
        print "plot_xy(): Added trajectory path %(trajectory)r." % vars()    
        return lines

    def remove_last_plot_xy(self, name="xy"):
        """Remove last individual x-y trajectory generated with plot_xy()."""
        import pylab
        l = self._lines[name].pop()
        l.remove()    
        trajectory = self._lines_annotation[name].pop()
        pylab.draw()
        print "remove_last_plot_xy(): Removed trajectory path %(trajectory)r." % vars()

    #
    #------------------------------------------------------------

    def block_iterator(self,blocksize=2**16,SQL=None,asrecarray=False):
        """Iterator that returns blocks of the selection of blocksize rows.

        block_iter = db.block_iterator(blocksize=2**16)
        for row_block in block_iter:
            weight = len(row_block)
            totsum += f(row_block) * weight
        
        """
        if SQL is None:
            SQL = """SELECT * FROM __self__"""
        _SQL = self._transform_SQL(SQL)
        cursor = self.connection.execute(_SQL)
        while True:
            rows = cursor.fetchmany(blocksize)
            if len(rows) == 0:
                raise StopIteration
            if asrecarray:
                try:
                    names = [x[0] for x in cursor.description]   # first elements are column names
                    rows = numpy.rec.fromrecords(rows,names=names)
                except:
                    print "block_iterator() ERROR: failed to return a recarray. Sorry, that's bad, I know."
                    raise
            yield rows
    
    # object properties
    def __len__(self):
        # use static __numrows if defined, otherwise compute it dynamically
        return self.__numrows or self.numrows

    def __iter__(self):
        _SQL = self._transform_SQL("""SELECT * FROM __self__""")
        # use iterators all way through
        for row in self.connection.execute(_SQL):
            yield row


class Selection(BaseSelection):
    """A selection of rows from the angle database.

    The idea is that one can operate on the selection and perform analysis such
    as plotting or generating wham input.

    Although one can iterate over the selection it is often more economical to
    simply get it as a numpy record array:

      s.recarray
    """

    #TODO: should be possible to use a selection instead of the db
    # (can be hacked in but perhaps there's a cleaner way)
    
    def __init__(self,db,sql=None,otherselfname=None,otherdataname=None,**kwargs):
        """A selection of rows from the angle database or view thereon.

        s = Selection(AngleDB, sql=None, otherselfname=None)

        s = Selection(angleDB, sql_WHERE_subclause>)
        s = Selection(angleDB, sql_SELECT_clause>)

        Arguments:
        db          database instance or Selection instance

        Keyword arguments:
        sql             sql statement [WHERE 1]
        otherselfname   selection will not access __self__ but table/view name
                        otherselfname instead (hack... mainly used internally)
        otherdataname   similarly, but for the __data__ view

        Example:
        
          s = Selection(angleDB, 'LID > 120 and LID < 130 and NMP < 50')          
          angles = Selection(angleDB, 'SELECT LID,NMP FROM __self__ ORDER BY LID ASC')

        The default selects everything.
        """
        import md5

        self.db = db
        super(Selection,self).__init__(db=db,**kwargs)

        if sql is None:
            sql = "1"        # WHERE clause that is always true: selects all

        # pretty unsafe... I hope the user knows what they are doing
        # - only read data to first semicolon
        # - here should be input scrubbing...
        safe_sql = re.match(r'(?P<SQL>[^;]*)',sql).group('SQL')

        if re.match(r'\s*SELECT.*FROM',safe_sql,flags=re.IGNORECASE):
            self.__sql = safe_sql
        else:
            # WHERE clause only        
            self.__sql = """SELECT * FROM __self__ WHERE """+str(safe_sql)
        # otherselfname is a hack to allow selections from selections that use __self__
        # (note: MUST replace __self__ and __data__ before md5!)
        self.__sql = self.__sql.replace('__self__', otherselfname or db.selfname)
        self.__sql = self.__sql.replace('__data__', otherdataname or db.dataname)
        self.selfname = 'VIEW_'+md5.new(self.__sql).hexdigest()  # unique name for view
        # create the view (note: no ? placeholders allowed!!)
        # IF NOT EXISTS may not work on older sqlite installations
        #SQL = """CREATE TEMP VIEW IF NOT EXISTS %s AS %s""" % (self.selfname, self.__sql)
        _SQL = """CREATE TEMP VIEW %s AS %s""" % (self.selfname, self.__sql)
        SQL = self._transform_SQL(_SQL)   # transforms __meta__ if necessary; __self__ already handled
        try:
            self.cursor.execute(SQL)
        except sqlite.OperationalError, err:
            #if err.message.find('already exists') > -1:  # only works for newer sqlite
            if " ".join(err.args).find('already exists') > -1:   # find phrase in all args of excpt
                pass  # workaround for older pysqlite that does not understand IF NOT EXISTS
            else:
                raise

        # __data__ view
        self.dataname = self.selfname + '_dataok'
        self._add_data_VIEW()

        # invariant statistics
        self.__numrows = self.SQL("""SELECT COUNT(*) FROM __self__""")[0][0]

    def selection(self,SQL):
        """Create a new selection from the SQL.

        selection(SQL)
        
        Should be a full SELECT clause or the subclause following WHERE.
        """
        return Selection(self.db,SQL,otherselfname=self.selfname,otherdataname=self.dataname)

    def sql():
        doc = "SQL string that defines the current selection. Read-only."
        def fget(self):
            return self.__sql
        return locals()
    sql = property(**sql())


    def numrows():
        doc = """Invariant size of the database selection (number of rows)."""
        def fget(self):
            return self.__numrows
        return locals()
    numrows = property(**numrows())

    def __repr__(self):
        return "<Selection %r with %d rows and columns %r>" % (self.selfname, self.numrows, self.columns)


class AngleDB(BaseSelection):
    """A SQL database with frames from AdK simulations.

    For each frame the source file and the LID and NMP angle are
    stored.

    Initialize the db with

       db = AngleDB(filename)

    If filename is the special name ':memory:' then the whole db will reside in
    RAM and be very fast but will be gone once the python session end (but see
    the clone() method and comments below how to save it).
       
    The database needs to be populated from angle.pickle files using
    'db.insert()' or 'db.insertmany()'. Changes are automatically
    commited to disk (using SQL commits).

    If the db is to be used for WHAM then the auxiliary trajectory
    database also needs to be populated (with 'db.add_metadata()') so
    that reference values and force constants are defined. See the
    docs for these methods for details.


    Adding data:

      db.insertmany()       populate the database with frames from files
      db.updatemany()       add additional frames (does not overwrite existing ones)

      db.add_metadata()     populate the meta data table (__meta__); this is a
                            prerequisite for any analysis
      db.load_energy()      load energy values from data files into db
      db.load_RMSD()        load RMSD values
      db.load_FRET()        load FRET distances
      db.load_saltbridge()  load salt bridge distances
      db.load_Contacts()    load q1-q2 data

    Useful methods:
    
       len(db)
       db.numrows         shows the number of entries
       db.filenames()     array of all filenames used

       db.SQL()           execute arbitrary SQL (see docs for details)
       db.clone()         save the db to new db file (see below)
       db.vacuum()        db maintenance (hardly ever needed)

       db.qrange()        methods to find frames matching certain criteria
       db.qaround()       for the angles

    To access all rows sequentially, simply iterate over db (but this is slow).

    To work with specific, user-defined selections, use the Selection
    class (see its docs).

    Building a big database can take hours on a slow disk (especially on a RAID
    5 array) because of write performance issues on those system. Alternatively,
    one can build the db in memory (:memory:) and then clone it to disk, using

       db.clone(filename)

    which should be significantly faster. Then one can later reuse the db from
    disk. However, note that the clone database ONLY CONTAINS THE STATUS OF THE
    in-memory db RIGHT BEFORE CLONING. ANY FURTHER CHANGES TO THE in-memory db
    ARE LOST WHEN THE python SESSION IS ENDED.

    Plotting Example:

    >>> db.plot(figname='figs/pmf/all_PMF_windows_combined_0.5deg_bins.png',cmap=cm.jet_r,bins_LID=160,title='all PMF windows combined')
    >>> xlim(40,75)
    >>> ylim(90,140)
    >>> savefig('figs/pmf/all_PMF_windows_combined_0.5deg_bins_limited.png')
        
    """
    # AngleDB is a special case of 'BaseSelection', see there. Note:
    #  __init__(),  SQL(),  selection() are special!!

    # these patterns are meant to be matched against a basename
    trj_patterns = (
        # for trajectories derived from x-tal/pdb structures: ignore initial 'co_':
        re.compile(r'(co|oc)_(?P<trjname>[0-9][a-zA-Z0-9][a-zA-Z0-9][a-zA-Z0-9].*?)(_pmf)?_angles\.pickle'),
        # for all other trajectories use the full name
        re.compile(r'(?P<trjname>.+?)(_pmf)?_angles\.pickle'),
        )

    # Matching of angle pickle files to trajectories is complicated because in
    # the beginning we did not name files uniformly; later we used
    # make_angles_adk.py (which standardizes names). This means that there are
    # quite a few quirky regular expressions and special cases to catch the
    # earlier names.
    #
    # Why do we allow files that do NOT contain pmf via  /(pmf)?/?  Because
    # we need the database for selecting starting frames. For WHAM and
    # umbrella sampling, build a different database that only contains
    # *_pmf_angles.pickle files.

    # Matching rules are applied in order in trj_patterns; first match wins:
    #
    # Rule 1
    # co_2ar7_a_1_pmf_angles.pickle           --> 2ar7_a_1
    # co_2ar7_a_1_long_3964_pmf_angles.pickle --> 2ar7_a_1_long_3964
    # co_1dvr_b_17_long_pmf_angles.pickle     --> 1dvr_b_17_long
    # 
    # Rule 2
    # oc_200_10_pmf_angles.pickle             --> oc_200_10

    # NOTE: renamed all pmf files that did not have a 'window_ID', eg
    #       svn mv co_2ar7_a_pmf_angles.pickle co_2ar7_a_1_pmf_angles.pickle

    def __init__(self,filename=":memory:",tempdir=None):
        """Initialize or load Angle database.

        db = AngleDB(filename, vacuum=True)

        filename       filename of a new or existing db on disk, or ':memory:';
                       if the file already exists then it is loaded
        tempdir        directory where the db keeps temporary tables; this should be
                       on a fast disk; None chooses the system default
        """
        self.connection = sqlite.connect(filename,
                                         detect_types=sqlite.PARSE_DECLTYPES | sqlite.PARSE_COLNAMES)
        # we need a REGEXP operator
        self.connection.create_function('regexp',2,
                                        lambda pattern,string: re.search(pattern,string) != None)
        self.connection.create_function('distance',4,
                                        lambda x,y,x0,y0: math.sqrt((x-x0)**2+(y-y0)**2))

        super(AngleDB,self).__init__(db=self,
            selfname="angles",metaname="trajectories",
            dataname="angles_dataok")
        
        self.filename = filename
         # observable groups in __self__: each group has its own loader class XXXLoader
        self.loaders = {'energy': EnergyLoader(self),
                        'RMSD': RMSDLoader(self),
                        'FRET': FRETLoader(self),
                        'saltbridge': SaltbridgeLoader(self),
                        'saltbridge_energy': SaltbridgeEnergyLoader(self),
                        'contacts': ContactsLoader(self),
                        }
        # add load_XXX methods to db
        for name,loader in self.loaders.items():
            self.__setattr__('load_'+name, loader.insertmany)
        self.__trajectories = None    # cache for trajectory names from meta

        try:
            # angles table (big main table)
            # This db is not normalized. JOINs are way too expensive
            # so we rather replicate data between __self__ and __meta__.
            # TODO: trajectory should not be here but in __meta__ (and filename should also not be
            #       here but a foreign key into __meta__.filename);
            #       right now filename is the primary db id
            # TODO: secondary observables such as energy or FRET distances should probably
            #       be in their own tables and connected via JOIN (makes it easier to modularize
            #       and extend ... maybe for the next project :-) )
            SQL = """-- SQL Database schema (non-normalized for speed)
            -- main table (currently also holds observables)
            -- NOTE: whenever a new observable is added one MUST add the columns here!
            CREATE TABLE %(selfname)s
                            (filename, trajectory,
                            TID CONSTRAINT fk_TID REFERENCES %(metaname)s (TID) ON DELETE SET NULL,
                            frame, LID, NMP,
                            energy,
                            cRMSD,oRMSD,DeltaRMSD,
                            FRET_Kern, FRET_Hanson, FRET_Sinev,
                            SB1, SB2, SB3, SB4, SB5, SB6, SB7, SB8, SB9, SB10, SB11, SB12, SB13, SB14,
                            ESB1,ESB2,ESB3,ESB4,ESB5,ESB6,ESB7,ESB8,ESB9,ESB10,ESB11,ESB12,ESB13,ESB14,
                            q1, q2, n1, n2);

            CREATE UNIQUE INDEX traj_frame ON %(selfname)s (trajectory,frame);
            CREATE INDEX filename ON %(selfname)s (filename);
            CREATE INDEX pos ON %(selfname)s (LID,NMP);

            -- Do these indices really help?
            CREATE INDEX has_energy ON %(selfname)s (trajectory,energy);
            CREATE INDEX has_rmsd ON %(selfname)s (trajectory,cRMSD);
            CREATE INDEX has_fret ON %(selfname)s (trajectory,FRET_Kern);
            CREATE INDEX has_saltbridge ON %(selfname)s (trajectory,SB1);
            CREATE INDEX has_saltbridge_energy ON %(selfname)s (trajectory,ESB1);
            CREATE INDEX has_contacts ON %(selfname)s (trajectory,q1);

            -- metadata table (small auxiliary table), essentially the wham meta file
            CREATE TABLE %(metaname)s
                     (TID INTEGER PRIMARY KEY,
                      filename UNIQUE, dcdpath UNIQUE, trajectory UNIQUE,
                      forceconstant, LID_ref, NMP_ref);
           
            -- trigger to mimick FOREIGN key constraint in __self__
            -- 'TID CONSTRAINT fk_TID REFERENCES %(metaname)s (TID) ON DELETE SET NULL,'
            CREATE TRIGGER fkd_TID
                     BEFORE DELETE ON %(metaname)s
                     FOR EACH ROW BEGIN
                         UPDATE %(selfname)s SET TID = NULL WHERE %(selfname)s.TID = OLD.TID;
                     END;
                     
             -- Tuning sqlite (PRAGMA is sqlite specific). See http://www.sqlite.org/pragma.html
             -- and http://web.utk.edu/~jplyon/sqlite/SQLite_optimization_FAQ.html
             -- try to speed up INSERTs etc:
             PRAGMA synchronous = OFF;
             -- have TMP DBs in memory (does this help or create problems?)
             PRAGMA temp_store = MEMORY;
                     """ % vars(self)
            self.cursor.executescript(SQL)
            self.connection.commit()
        except sqlite.OperationalError,error:
            self.connection.rollback()
            errmsg = str(error)  # ('error.message' fails on slightly older versions)
            if errmsg.find('table %(selfname)s already exists' % vars(self)) == -1:
                # an error we cannot ignore
                raise
            self.cursor.execute("SELECT COUNT(*) FROM %(selfname)s" % vars(self))
            nrows, = self.cursor.fetchone()
            print "__init__(): Using existing database '%(filename)s' with %(nrows)r entries." % vars()

        # set TEMP directory: should be on a fast disk
        self.tempdir = None    # default
        if tempdir is None:
            pass
        elif os.path.exists(tempdir):
            self.tempdir = os.path.realpath(tempdir)
        else:
            warnings.warn("The supplied tempdir = %(tempdir)r does not exist; using default." % vars())
        if self.tempdir is None:
            self.cursor.execute("PRAGMA temp_store_directory = ''")  # set system default
        else:
            # WARNING: this input is NOT safe, see http://xkcd.com/327/
            print "__init__(): using tempdir = %(tempdir)r for temporary tables" % vars(self)
            self.cursor.execute("PRAGMA temp_store_directory = '%s'" % self.tempdir)            
            
        # temporary=False creates the view in memory (I think) and that
        # can blow up
        print "__init__(): adding __data__ virtual table"
        self._add_data_VIEW(temporary=True)

        self.connection.commit()   # not sure if this is needed? Probably not.
        print "__init__(): db is ready and pretty"

    def vacuum(self):
        """Optimize the database with the VACUUM command. Can take a while."""
        self.connection.execute("VACUUM")
        
    def clone(self,filename):
        """Copy the current database and save it to the given file.

        db.clone(filename)

        Mainly useful to store the :memory: database on disk for
        reusal. The cloned file can be loaded later with

          db = AngleDB(filename)

        Note that only content present in the original database up to
        the cloning operation is stored in the cloned db; it does not
        automatically update.

        If the db in filename already exists, data will be added but
        overwrite existing content (uses 'INSERT OR REPLACE').

        One cannot clone to the database that is currently loaded.
        """
        if filename == self.filename:
            raise ValueError("filename must be different from the original one.")

        print "clone():    creating/loading target database %(filename)r" % vars()
        clone_db = AngleDB(filename)  # just create empty db or load existing one
        del clone_db

        print "clone():    copying content %r --> %r ..." % (self.filename, filename)
        self.connection.commit()
        self.connection.execute("""ATTACH DATABASE ? AS clone""", (filename,))
        SQL = self._transform_SQL("""
        INSERT OR REPLACE INTO clone.__self__ SELECT * FROM __self__ ORDER BY ROWID;
        INSERT OR REPLACE INTO clone.__meta__ SELECT * FROM __meta__ ORDER BY ROWID;
        """)
        self.connection.executescript(SQL)
        self.connection.commit()
        self.connection.execute("""DETACH DATABASE clone""")
        print "clone():    done"

    def selection(self,SQL):
        # note that first argument of Selection is self!
        return Selection(self,SQL,otherselfname=self.selfname)
    selection.__doc__ = Selection.selection.__doc__

    def has_data(self,trajectoryname):
        """Returns True if trajectoryname has meta data."""
        return trajectoryname in self.trajectories

    # TODO:
    # I can generate/template the has_XXX in __init__: just check the first Loader.column[0]
    def has_energy(self,trajectoryname):
        """Returns True if trajectoryname in the db contains energies."""
        return self.SQL("SELECT COUNT(energy) FROM __self__ WHERE trajectory = ? AND NOT energy ISNULL",
                        trajectoryname)[0][0] > 0

    def has_FRET(self,trajectoryname):
        """Returns True if trajectoryname in the db contains FRET distances."""
        return self.SQL("SELECT COUNT(FRET_Kern) FROM __self__ WHERE trajectory = ? AND NOT FRET_Kern ISNULL",
                        trajectoryname)[0][0] > 0

    def has_saltbridge(self,trajectoryname):
        """Returns True if trajectoryname in the db contains saltbridge distances."""
        return self.SQL("SELECT COUNT(SB1) FROM __self__ WHERE trajectory = ? AND NOT SB1 ISNULL",
                        trajectoryname)[0][0] > 0

    def has_saltbridge_energy(self,trajectoryname):
        """Returns True if trajectoryname in the db contains saltbridge energy."""
        return self.SQL("SELECT COUNT(ESB1) FROM __self__ WHERE trajectory = ? AND NOT ESB1 ISNULL",
                        trajectoryname)[0][0] > 0

    def has_RMSD(self,trajectoryname):
        """Returns True if trajectoryname in the db contains RMSD data."""
        return self.SQL("SELECT COUNT(cRMSD) FROM __self__ WHERE trajectory = ? AND NOT cRMSD ISNULL",
                        trajectoryname)[0][0] > 0

    def has_contacts(self,trajectoryname):
        """Returns True if trajectoryname in the db contains contacts (q1/q2) data."""
        return self.SQL("SELECT COUNT(q1) FROM __self__ WHERE trajectory = ? AND NOT q1 ISNULL",
                        trajectoryname)[0][0] > 0

    def insert(self,filename,replace=True,commit=True):
        """Insert data from pickled angle file into db.
        The replace=False option skips any data whose trajectory name is already in the db.
        """

        trajectoryname = None
        for P in self.trj_patterns:
            m = P.match(os.path.basename(filename))
            if m:
                trajectoryname = m.group("trjname")
                print "%(filename)s\n\t ---> '%(trajectoryname)s'" % vars()
                break
        if trajectoryname is None:
            raise RuntimeError("Cannot identify possible trajectory name from which %s is derived." % filename)

        if not replace and self.has_data(trajectoryname):
            print "\t    > %(trajectoryname)r already has angle data, skipped" % vars()
            return False   # will count trajectory as not_inserted
        
        angles = numpy.load(filename)

        # TODO: add data to __meta__ right away and use this as foreign key
        #       in __self__ in place of filename
        SQL = self._transform_SQL("""INSERT OR REPLACE INTO __self__ 
                                     (filename, trajectory, frame, LID, NMP) VALUES (?,?,?,?,?)""")
        self.cursor.executemany(SQL, [(filename,trajectoryname,frame+1,float(LID),float(NMP)) 
                                      for frame,(LID,NMP) in enumerate(angles)] )
        if commit:
            self.connection.commit()
        print "\t ---> read %d frames, inserted %d into table" % (len(angles), self.cursor.rowcount)
        return self.cursor.rowcount > 0        

    def insertmany(self,filepattern,replace=True):
        """Insert all files that match the glob pattern."""
        
        filenames = glob.glob(filepattern)
        nsuccess = 0
        not_inserted = []        
        if len(filenames) == 0:
            warnings.warn("Found no files with pattern '%(filepattern)s'." % vars())
            return {'N_files':len(filenames), 'N_inserted':nsuccess, 'not_inserted':not_inserted}
        try:
            for filename in filenames:
                # commit=False is crucial for performance (~ x10) to avoid a high write penalty;
                # the whole transaction is commited at the end of the loop
                # see http://www.sqlite.org/faq.html#q19 for 'INSERT is slow'
                # (perhaps also try 'PRAGMA synchronous=OFF'?)
                success = self.insert(filename,replace=replace,commit=False)
                if success:
                    nsuccess += 1
                else:
                    not_inserted.append(filename)
        except:
            self.connection.rollback()
            raise
        self.connection.commit()   # write the whole transaction to the db
        return {'N_files':len(filenames), 'N_inserted':nsuccess, 'not_inserted':not_inserted}

    def load_angles(self,filepattern,**kwargs):
        """Insert all angle files that match the glob pattern and add metadata.

        load_angles(filepattern,replace=True,**kwargs)

        filepattern      shell glob pattern that finds angle.pickle files
        replace          True: overwrite existing entries in db
                         False: keep existing entries (faster)  [True]
        **kwargs         keyword args for db.add_metadata()
        """
        # load_angles()  makes the interface more consistent (cf load_XXX() methods)
        result = self.insertmany(filepattern,replace=kwargs.pop('replace',True))
        self.add_metadata(**kwargs)
        return result

    def updatemany(self,filepattern):
        """Insert all files matching filepattern IF they are not in the db already."""
        return self.insertmany(filepattern,replace=False)


    def add_metadata(self,include_all=True,
                     reftxt_pattern=os.path.join(config.basedir,'pmf','*.txt'),
                     autometa_pattern=os.path.join(config.basedir,'pmf','autoumbrella','jobs','*','windows.autometa'),
                     forceconst_file=os.path.join(config.basedir,'pmf','cons_value.txt'),
                     blacklist_file=os.path.join(config.basedir,'pmf','blacklist.txt')):
        """Add the metadata for umbrella windows to the __meta__ table. Only required for PMF.wham.

        db.add_metadata(include_all=False,**kwargs)

        Arguments:
        include_all       True:  If a file is not found in the *.txt files then it is ASSUMED to be
                                 a equilibrium simulation and enetered in the table with NULL for
                                 the force constant and reference values
                          False: Only enter recognized umbrella windows.

                          The default is True. include_all=False can be used when generating
                          umbrella windows using PMF.umbrella but is not ncessary anymore.

        blacklist_file    uses the default pmf/blacklist.txt, but can be set to None to not
                          exclude any file from the database
                          
        Typically no other kwargs are necessary as appropriate defaults are chosen.
        """
        # TODO: change build process:
        #  1. load metadata maps
        #  2. load trajectories; for each trajectory added to __self__
        #     also add available metadata to __meta__; reject
        #     trajectories that cannot be matched to metadata;
        #     warn/reject duplicates
        #     reject blacklisted ones right away (configurable)        
        
        # Because __meta__ is used with PMF.umbrella and self.add_dcds() we also
        # need to add information for a AngleDB containing equilibrium
        # simulations.  An equilibrium simulation has 'forceconstant <-- NULL'
        # so that 'WHERE forceconstant ISNULL' can be used to find the
        # equilibrium sims in the table.
        #
        # Any frame in __self__ which is referenced in __meta__ has
        # its TID column set to the TID (trajectory ID) of the
        # entry in __meta__; eventually this could be used as a
        # foreign key and make [filename] superfluous in __self__.

        n_missing = 0             # count of all files that were not found via the txt files
        n_matched = 0             # count of files matched via txt files
        n_blacklisted = 0         # count of removed files via blacklist.txt

        # try the default
        print "Loading reference values from %(reftxt_pattern)s." % vars()
        refmap = ReferenceValuesMap(reftxt_pattern)

        print "Loading metadata from %(autometa_pattern)s." % vars()
        autometamap = AutometaMap(autometa_pattern)
        
        print "Loading force constants from %(forceconst_file)s." % vars()
        fcmap = ForceConstantsMap(forceconst_file)

        print "Loading blacklist from %(blacklist_file)s." % vars()
        blacklistmap = BlacklistMap(blacklist_file)
        
        print "Finding all filenames in database (and excluding the ones in the blacklist)"
        if include_all:
            print "All files not listed explicitly are classified as UNBIASED simulations."        
        print "Matching filenames with force constants and reference values..."
        trjnames = self.SQL("""SELECT filename,trajectory FROM __self__ GROUP BY filename""")
        reclist = []
        for (t,trajectory) in trjnames:
            if blacklistmap.match(t):
                print "Ignoring/deleting blacklisted %(t)s." % vars()
                # clean it from db if it's already there!
                # Note that this runs a trigger on __self__ to set TID -> None
                # and hence can take very long!
                self.SQL("DELETE FROM __meta__ WHERE GLOB(?,filename)", t)
                n_blacklisted += 1
                continue
            try:
                try:
                    forceconstant,NMP_ref,LID_ref = autometamap.match(t)
                except RuntimeError:
                    forceconstant = fcmap.match(t)
                    LID_ref, NMP_ref = refmap.match(t)
                reclist.append( (t,trajectory,forceconstant,LID_ref,NMP_ref) )
                n_matched += 1
            except RuntimeError:
                n_missing += 1
                if include_all:
                    reclist.append( (t,trajectory,None,None,None) )
                else:
                    print "Missing reference values for '%(t)s'." % vars()
        SQL = self._transform_SQL("""INSERT OR REPLACE INTO __meta__ 
                 (filename,trajectory,forceconstant,LID_ref,NMP_ref) VALUES (?,?,?,?,?)""")
        try:
            self.cursor.executemany(SQL, reclist)
            self._mark_frames_with_metadata()
        except:
            self.connection.rollback()
            raise
        self.connection.commit()  # only commit when all is done
        del self.trajectories     # cache trajectory names (del dirties the cache)
        print "In total %(n_blacklisted)d files were initially removed by blacklisting." % vars()
        print "In total %(n_matched)d files were matched against metadata and txt files." % vars()
        if include_all:
            print "In total %(n_missing)d files were classified as UNBIASED simulations (include_all=%(include_all)r)." % vars()
        else:
            print "In total %(n_missing)d files are missing reference values and force constants." % vars()

    def _mark_frames_with_metadata(self):
        """For ALL frames, fill metadata column."""
        # Is this function a problem? Something takes forever and consumes memory -- maybe this?
                
        print "Marking frames that are valid (and marking the rest as blacklisted)..."

        # this adds a TID to __self__ for each trajectory that is in __meta__
        SQL = """UPDATE __self__
                  SET TID = (SELECT __meta__.TID
                  FROM __meta__
                  WHERE __meta__.trajectory = __self__.trajectory)
                  WHERE EXISTS (SELECT __meta__.trajectory
                                FROM __meta__
                                WHERE __meta__.trajectory = __self__.trajectory)
                                """
        self.SQL(SQL)
        # and the fkd_TID trigger makes sure that deleting an entry from
        # __meta__ immediately NULLs the corresponding frames in __self__

    def delete_metadata(self,*patterns):
        """Delete entries from __meta__ for which patterns match the trajectory.

        delete_metdata(pattern1,pattern2,...)

        A pattern is a shell-style glob pattern, i.e. it can contain '*',
        '?'. '[0-9]', etc. The trajectory fields contains the basename. See
        db.trajectories for the current list in __meta__.
        """
        _SQL = """DELETE FROM __meta__ WHERE GLOB(?,trajectory)"""
        SQL = self._transform_SQL(_SQL)
        self.connection.executemany(SQL,[(p,) for p in patterns])
        self.commit()
        ndel = self.cursor.rowcount
        del self.trajectories   # dirty cache
        print "Deleted %(ndel)d entries from __meta__"
        return ndel > 0

            
    def add_dcds(self,*filepatterns):
        """Fill dcdpath field in the __meta__ table; only required for PMF.umbrella.

        db.add_dcds('~/Projects/AdK_external/pmf/*.dcd','../testing/pmf/*.dcd')
        
        DCDs are specified as filenames or as shell-style glob patterns
        (using '*' or '[0-9]'). They are matched against the 'trajectory'
        field in the __self__ table and entered in the __meta__ table. Hence one
        must run db.add_metadata() before running this method.

        The matching is done against the regular expression

          '.*<trajectory>(_pmf)?\.dcd'

        (The <trajectory> identifier has been stripped of '_pmf' but
        some dcds contain it, others don't.)

        If no __meta__ table is found we run

          add_metadata()

        ourselves.
        """

        n_tot = self.numsimulations
        if n_tot == 0:
            warnings.warn("__meta__ table is empty; trying to add metadata now WITH EQUILIBRIUM simulations...")
            self.add_metadata(all_include=True)
            n_tot = self.numwindows
            if n_tot == 0:
                raise RuntimeError("Failed to add meta data. You must manually fix this problem.")
                
        dcdlist = []
        for pat in filepatterns:
            dcdlist.extend(glob.glob(os.path.expanduser(os.path.expandvars(pat))))
        if len(dcdlist) == 0:
            warnings.warn("No dcds were found; provide the correct paths.")
            return

        print "Matching actual dcd filenames against filenames in __meta__..."
        # make sure to start with clean temp table
        try:
            self.SQL("""DROP TABLE dcdlist""")
        except sqlite.OperationalError:
            pass     
        self.SQL("""CREATE TEMP TABLE dcdlist (dcdpath UNIQUE)""")
        self.cursor.executemany('INSERT OR IGNORE INTO dcdlist (dcdpath) VALUES (?)',
                                [(x,) for x in dcdlist])
        try:
            rows = self.SQL("""SELECT M.filename,T.trajectory,D.dcdpath
                        FROM __meta__ AS M
                        NATURAL LEFT JOIN (SELECT filename,trajectory FROM __self__ GROUP BY filename) AS T
                        LEFT JOIN dcdlist AS D
                        WHERE REGEXP('.*'||T.trajectory||'(_pmf)?\.dcd',D.dcdpath)
                        """, asrecarray=True)
            # no idea how to do this as a single SQL statement -- see _mark_frames_with_metdata()...
            SQL = self._transform_SQL("""UPDATE __meta__ SET dcdpath = ? WHERE filename = ?""")
            self.cursor.executemany(SQL, [(row.dcdpath,row.filename) for row in rows])
            self.connection.commit()
            print "Added %r dcd paths to __meta__." % self.cursor.rowcount
        finally:
            pass  # leave it for debugging
            #self.SQL('DROP TABLE dcdlist')
        n_missing = self.SQL('SELECT COUNT(*) FROM __meta__ WHERE dcdpath ISNULL')[0][0]
        n_dcdpath = n_tot - n_missing
        print "Have %(n_dcdpath)d dcd paths out of %(n_tot)d entries in __meta__." % vars()
        if n_missing > 0:
            print "Still missing %(n_missing)d paths." % vars()

    def purge(self):
        """Remove entries from the database that have been blacklisted.

        When the database becomes big it is advantageous to trim its size by doing

          db.add_metadata()
          db.purge()

        NOTE: This removes any entry that does not have an entry in __meta__. Use with care.
        """
        raise NotImplemented

    def numwindows():
        doc = """Number of unique umbrella windows."""
        def fget(self):
            # forceconstant ISNULL is marker for equilibrium simulation
            return self.SQL("""SELECT COUNT(*) FROM __meta__
                                WHERE NOT filename ISNULL AND NOT forceconstant ISNULL""")[0][0]
        return locals()
    numwindows = property(**numwindows())

    def numsimulations():
        doc = """Number of entries in the __meta__ table"""
        def fget(self):            
            return self.SQL("""SELECT COUNT(*) FROM __meta__
                                WHERE NOT filename ISNULL""")[0][0]
        return locals()
    numsimulations = property(**numsimulations())

    def trajectories():
        doc = "trajectory names currently held in __meta__; del dirties the cache"
        def fget(self):
            if self.__trajectories is None:   # cache is dirty
                self.__trajectories = [x[0] for x in self.SQL("SELECT trajectory FROM __meta__ ORDER BY trajectory ASC")]
            return self.__trajectories
        def fdel(self):
            self.__trajectories = None
        return locals()
    trajectories = property(**trajectories())

    def __del__(self):
        """When the object is destroyed we make sure to get all changes in."""
        self.connection.commit()
        self.connection.close()

    def __repr__(self):
        return "<AngleDB %r, main table %r with %d rows and columns %r>" % \
               (self.filename, self.selfname, len(self), self.columns)


class AutometaMap(object):
    """Rules to assign reference values and force constant from a autometa file (see PMF.umbrella).

    Format (defined in PMF.umbrella.Autometa.write():

      # comment line (ignored)
      taskid   pdbname  forceconstant  NMP_ref  LID_ref   dcdname

    The pickle files are named BASEDCDNAME_angles_pmf.pickle (dcdname
    eg 'liz1_185.dcd', basename is then 'liz1_185'))

      
    """

    def __init__(self,globpattern=os.path.join(config.basedir,'pmf','autoumbrella','jobs','*','windows.autometa')):
        """Load exact matching patterns for all meta data from windows autometa files."""
        # This method is kept superficially similar to the ReferenceValuesMap
        # and ForceconstantValueMap; it is not clear that we really need
        # regular expression matching but I'll keep it just in case.
        
        files = glob.glob(globpattern)
        if len(files) == 0:
            warnings.warn("No files found with pattern '%s'." % globpattern)
        reclist = []
        for AMfilename in files:
            autometa = open(AMfilename,'r')
            try:
                for line in autometa:
                    line = line.strip()
                    if len(line) == 0 or line.startswith('#'):
                        continue
                    taskid,pdbname,forceconstant,NMP_ref,LID_ref,dcdname = line.split()
                    dcdbase,ext = os.path.splitext(dcdname)
                    regex = dcdbase+"_pmf_angles\.pickle"
                    compiled_regex = re.compile(regex)
                    reclist.append( (regex,float(forceconstant),float(NMP_ref),float(LID_ref),
                                     compiled_regex) )
            finally:
                autometa.close()

        self.regexmap = numpy.rec.fromrecords(reclist, names='regex,forceconstant,NMP_ref,LID_ref,compiled_regex')

    def match(self,filename):
        """Match filename against the map and return the corresponding angle reference values (LID,NMP)."""
        for regex,forceconstant,NMP_ref,LID_ref,compiled_regex in self.regexmap:
            if compiled_regex.search(filename):   # use search to catch all trajectories
                return forceconstant,NMP_ref,LID_ref
        raise RuntimeError('AutometaMap: No match for %s was found.' % filename)
                
            

class ReferenceValuesMap(object):
    """Rules to assign the LID angle and NMP angle reference values to a trajectory filename.

    Rules are read from the pmf/*.txt files.


    The trajectory name portion is encoded in the filename; the file
    itself contains the numbering of the simulation and the reference
    value:

     (?P<trjname>.*)\.txt:  windowID   LID_0  NMP_0  --> <trjname>_int(<windowID>)

    NOTE: all pickle files must match

       .*_pmf_angles\.pickle
       
    """

    trjname_pattern = re.compile(r'(.*/)*(?P<trjname>.*)\.txt')
    ignored_txt = ['cons_value', 'blacklist']     # hack so that one can easily glob *.txt

    def __init__(self,globpattern=os.path.join(config.basedir,'pmf','*.txt')):
        """Load rules from all files matching globpattern."""
        files = glob.glob(globpattern)
        if len(files) == 0:
            warnings.warn("No files found with pattern '%s'." % globpattern)
        reclist = []
        for reffilename in files:
            m = self.trjname_pattern.search(reffilename)   # use search to catch all trajectories
            if m:
                trjbasename = m.group('trjname')
                print "%(reffilename)s --> %(trjbasename)r" % vars()
            else:
                trjbasename = None
                continue
            # hack: exclude the cons_value.txt and blacklist.txt file
            if trjbasename in self.ignored_txt:
                print "... ignored"
                continue
            # hack: co200.txt --> co_200_<window_ID>_pmf_angles.pickle
            baseregex = trjbasename.replace('oc','oc_?')
            baseregex = baseregex.replace('co','co_?')                    

            regex_filename = baseregex + '_pmf_angles\.pickle'
            # regex_filename matches directly on the filename of the txt file
            # txt:    2rh5_b_012_long1657.txt
            # pickle: co_2rh5_b_012_long1657_pmf_angles.pickle
            # Allow alternative file format in which:
            # filename.txt --> filename.pickle and window_ID in file is ignored

            regex_windowID_template = "%(baseregex)s_0*%(window_id)d_pmf_angles\.pickle"
            # should match eg 
            #    co100.txt -->  co_co100_3_pmf_angles.pickle
            #    co100.txt -->  co_co100_003_pmf_angles.pickle
            #    oc200.txt -->  oc_200_10_pmf_angles.pickle
            if trjbasename.find('_long') > -1:
                # special hack for filenames with 'long' and WITH MULTIPLE window_IDs
                #    2ar7_b_long.txt -->
                #      co_2ar7_b_7_long_pmf_angles.pickle
                #      co_2ar7_b_10_long_pmf_angles.pickle
                baseregex = baseregex.replace('_long','')
                regex_windowID_template = "%(baseregex)s_0*%(window_id)d_long_pmf_angles\.pickle"

            reffile = open(reffilename,'r')
            try:
                for line in reffile:
                    line = line.strip()
                    if len(line) == 0 or line.startswith('#'):
                        continue
                    fields = line.split()
                    window_id, LID_ref, NMP_ref = int(fields[0]), float(fields[1]), float(fields[2])
                    # regular expression against which the pickle filename is searched:
                    #regex_windowID = baseregex + '_0*' + str(window_id) + '_pmf_angles\.pickle'
                    regex_windowID = regex_windowID_template % vars()
                    regex = "(" + regex_windowID + ")|(" + regex_filename +")" # checked in order??
                    reclist.append( (regex, LID_ref, NMP_ref, re.compile(regex), reffilename) )
            finally:
                reffile.close()
        self.regexmap = numpy.rec.fromrecords(reclist, names='regex,LID_ref,NMP_ref,compiled_regex,file')

    def match(self,filename):
        """Match filename against the map and return the corresponding angle reference values (LID,NMP)."""
        for regex,LID_ref,NMP_ref,compiled_regex,fn in self.regexmap:
            if compiled_regex.search(filename):   # use search to catch all trajectories
                return LID_ref, NMP_ref
        raise RuntimeError('ReferenceValuesMap: No match for %s was found.' % filename)
    

class ForceConstantsMap(object):
    """Rules to assign a force constant to a trajectory file.

    Rules are read in from pmf/cons_value.txt

    file format:
       comments start with '#' and are ignored
       white lines are ignored
       REGEX                value   # REGEX is a regular expression pattern with `.*', `[A-Z]', ...
                                    # value is a force constant in unit(?) of Charmm
                                    Spaces are not allowed in REGEX.
       'DEFAULT'            value   # gives the default for any trajectories not listed, equiv. to `.*'
     
    REGEX patterns  are applied in order found in the file and the first one matching is
    selected; if no pattern matches, the 'DEFAULT' value is taken.
    """
    def __init__(self,filename=os.path.join(config.basedir,'pmf','cons_value.txt')):
        """Load matching rules for force constants from file."""
        self.filename = filename
        self.regexmap = None
        fcfile = open(filename,'r')
        reclist = []
        try:
            for line in fcfile:
                line = line.strip()
                if len(line) == 0 or line.startswith('#'):
                    continue
                regex, forceconstant = line.split()
                if regex.upper() == 'DEFAULT':
                    # delay until read all others
                    DEFAULT_forceconstant = float(forceconstant)
                    continue
                compiled_regex = re.compile(regex)
                reclist.append( (regex, float(forceconstant), compiled_regex) )
        finally:
            fcfile.close()
        reclist.append( ('.*', DEFAULT_forceconstant, re.compile('.*')) )  # DEFAULT *MUST* come last
        self.regexmap = numpy.rec.fromrecords(reclist,names='regex,forceconstant,compiled_regex')

    def match(self,filename):
        """Match filename against the map and return the corresponding force constant."""
        for regex,forceconstant,compiled_regex in self.regexmap:
            if compiled_regex.search(filename):   # use search to catch all trajectories
                return forceconstant
        raise RuntimeError('ForceConstantsMap: No match for %s was found. Have you defined a DEFAULT entry?' % filename)

class BlacklistMap(object):
    """Rules to ignore trajectories.

    Rules are read in from pmf/blacklist.txt

    file format:
       comments start with '#' and are ignored
       white lines are ignored
       REGEX            # REGEX is a regular expression pattern with `.*', `[A-Z]', ...
                          Spaces are not allowed in REGEX.
     
    REGEX patterns  are applied in order found in the file and the first one matching is
    selected.
    """
    def __init__(self,filename=os.path.join(config.basedir,'pmf','blacklist.txt')):
        """Load matching rules to blacklist from file; never match if filename is None"""
        self.filename = filename
        self.regexmap = None
        if self.filename is None:
            return   # no blacklist required
        blacklistfile = open(filename,'r')
        reclist = []
        try:
            for line in blacklistfile:
                line = line.strip()
                if len(line) == 0 or line.startswith('#'):
                    continue
                regex = line.split()[0]
                compiled_regex = re.compile(regex)
                reclist.append( (regex, compiled_regex) )
        finally:
            blacklistfile.close()
        self.regexmap = numpy.rec.fromrecords(reclist,names='regex,compiled_regex')

    def match(self,filename):
        """Match filename against the map and return the corresponding force constant."""
        if self.regexmap is None:
            return False    # no blacklist provided
        matched = False
        for regex,compiled_regex in self.regexmap:
            if compiled_regex.search(filename):   # use search to catch all trajectories
                matched = True
                break
        return matched


class ObservableLoader(object):
    """Helper class to insert data from data files. Override to customize."""

    columns = ()      # override with the names of the columns in AngleDB __self__;
                      # note read_datafile_iter() specs!!
    filename_patterns = (re.compile(r'(?P<trjname>.+?)(_pmf)?_.*\.dat(\.bz2)?'),)   # override

    def __init__(self,db=None,**kwargs):
        self.db = db

    def has_observable(self,trajectoryname):
        # override with eg 'return self.db.has_energy(trajectoryname)'
        return False

    def read_datafile_iter(self,filename):
        # return a tuple which MUST follow this API:
        #  (col1, col2, ..., colN, frame)
        # col1 .. colN: self.columns (see __init__)
        # frame:            frame number (integer, starts at 1)
        raise NotImplementedError('Override so that it yields a data record.')

    def datalines(self,datafile):
        """Iterator yielding lines from the opened file that are data."""
        for line in datafile:
            line = line.strip()
            if line.startswith('#') or len(line) == 0:
                continue
            yield line

    def SET_columns_sql(self):
        """Return SQL string which is the 'SET col1 = ?, col2 = ?, ...' part of the UPDATE."""
        return "SET " + ", ".join(['%s = ?' % col for col in self.columns])
        
    def insert(self,filename,replace=True,commit=True):
        """Insert data from observable file into db.
        The replace=False option skips any data whose trjactory name is already in db
        """
        
        trajectoryname = None
        for P in self.filename_patterns:
            m = P.match(os.path.basename(filename))
            if m:
                trajectoryname = m.group("trjname")
                print "%(filename)s\n\t ---> '%(trajectoryname)s'" % vars()
                break
        if trajectoryname is None:
            raise RuntimeError("Cannot identify possible trajectory name from which %r is derived." % filename)

        if not replace and self.has_observable(trajectoryname):
            print "\t    > %(trajectoryname)r already has observable data, skipped" % vars()
            return False   # will count energies as not_inserted
        
        # sanity check
        nmatches = self.db.SQL("""SELECT COUNT(*) FROM __meta__ WHERE trajectory = ?""", trajectoryname)[0][0]
        if nmatches == 1:
            pass        # please only match a single entry!
        elif nmatches == 0:
            print "\t !!!> %(trajectoryname)r is not in __meta__, skipped" % vars()
            return False    # shortcut; no need to go through the remainder
        else:
            raise RuntimeError("Too many matches (%(nmatches)d) for %(filename)r --> %(trajectoryname)r." % vars())

        # ATTENTION: self.read_datafile_iter's records and self.columns must be in sync!!!
        #            Also see the notes for the class.
        SQL = self.db._transform_SQL("UPDATE OR REPLACE  __self__ " +
                                     self.SET_columns_sql() + 
                                     "WHERE trajectory = '%s'" % trajectoryname +
                                     "AND frame = ?")
        self.db.cursor.executemany(SQL, self.read_datafile_iter(filename))
        if commit:
            self.db.connection.commit()
        print "\t ---> inserted %d frames into table" % (self.db.cursor.rowcount,)
        return self.db.cursor.rowcount > 0

    def insertmany(self,filepattern,replace=True):
        """Insert all observables from files that match the glob pattern."""

        filenames = glob.glob(filepattern)
        nsuccess = 0
        not_inserted = []        
        if len(filenames) == 0:
            warnings.warn("Found no files with pattern '%(filepattern)s'." % vars())
            return {'N_files':len(filenames), 'N_inserted':nsuccess, 'not_inserted':not_inserted}
        try:
            for filename in filenames:
                # commit=False is crucial for performance (~ x10) to avoid a high write penalty;
                # the whole transaction is commited at the end of the loop
                success = self.insert(filename,replace=replace,commit=False)
                if success:
                    nsuccess += 1
                else:
                    not_inserted.append(filename)
        except:
            self.db.connection.rollback()
            raise
        self.db.connection.commit()   # write the whole transaction to the db
        return {'N_files':len(filenames), 'N_inserted':nsuccess, 'not_inserted':not_inserted}


    def updatemany(self,filepattern):
        """Insert all observables matching filepattern IF they are not in the db already.

        NOTE: run 'db.add_metadata()' before the update
        """
        return self.insertmany(filepattern,replace=False)

    

class EnergyLoader(ObservableLoader):
    """Insert energy into db.
    File format (white space separated); ignore angles and match on name:

      # comment
      frame  energy  LID NMP
    """
    
    columns = ('energy',)
    filename_patterns = (
        # trjname ---> dcd from which data were generated; try matching against
        # __self__.trjname because dcdpath in __meta__ is not available in most
        # cases
        
        # standard pattern: all use that because this is what make_energy.py spits out
        # (CHECK: although: we seem to need to remove the _pmf part... not sure where I chop this
        # when adding the 'trajectory' column... QUICK FIX:)
        re.compile(r'(?P<trjname>.+?)(_pmf)?_energy\.dat(\.bz2)?'),
        )

    def __init__(self,db=None,**kwargs):
        super(EnergyLoader,self).__init__(db=db,**kwargs)

    def has_observable(self,trajectoryname):
        return self.db.has_energy(trajectoryname)

    def read_datafile_iter(self,filename):
        """Return an iterator over records from the energy file."""

        datafile = bz2.BZ2File(filename,'r')
        try:
            for line in self.datalines(datafile):
                frame,energy,LID,NMP = line.split()
                yield (float(energy),int(frame))
        finally:
            datafile.close()

class FRETLoader(ObservableLoader):
    """Insert FRET into db.

    File format (white space separated); ignore time (just count lines to get
    the frame number) and match on name:

      # comment
      time/ps  FRET_Kern FRET_Hanson FRET_Sinev
    """
    
    columns = ('FRET_Kern','FRET_Hanson','FRET_Sinev')
    filename_patterns = (
        # (modelled after energy_patterns)
        # standard pattern: all use that because this is what make_fret.py spits out
        re.compile(r'(?P<trjname>.+?)(_pmf)?_fret\.dat(\.bz2)?'),
        )    
    
    def __init__(self,db=None,**kwargs):
        super(FRETLoader,self).__init__(db=db,**kwargs)

    def has_observable(self,trajectoryname):
        return self.db.has_FRET(trajectoryname)

    def read_datafile_iter(self,filename):
        """Return an iterator over records from the FRET file."""

        datafile = bz2.BZ2File(filename,'r')
        frame = 0   # count frames (1-based)
        try:
            for line in self.datalines(datafile):
                frame += 1
                time_ps,FRET_Kern,FRET_Hanson,FRET_Sinev = map(float,line.split())
                yield (FRET_Kern,FRET_Hanson,FRET_Sinev,frame)
        finally:
            datafile.close()


class SaltbridgeLoader(ObservableLoader):
    """Insert saltbridge data into db.

    File format (white space separated); ignore time (just count lines to get
    the frame number) and match on name:

      # comment
      time/ps  time/ps  SB1 SB2 ... SB10

      # SALT BRIDGE DISTANCES IN AKECO (SEE "txt/salt-bridge_info.txt")
      # WRITTEN BY $ID: SALTBRIDGE_DISTANCES.INP 3119 2009-03-18 21:32:31Z DENNIEJ0 $
      # ALL DISTANCES IN ANGSTROM.
      # 1:D33-R156  2:K97-N190  3:D54-K157  4:D54-R156  5:D54-R167
      # 6:R36-D158  7:R36-E170  8:K57-D158  9:K57-E170 10:D118-K136
      # 11:D158-R36 12:D84-K13  13:E185-K97  14:E44-K47
    """
    
    columns = ('SB1','SB2','SB3','SB4','SB5','SB6','SB7','SB8','SB9','SB10',
               'SB11','SB12','SB13','SB14')
    filename_patterns = (
        # standard pattern: all use that because this is what make_saltbridges.py spits out
        re.compile(r'(?P<trjname>.+?)(_pmf)?_saltbridges\.dat(\.bz2)?'),
        )
    
    def __init__(self,db=None,**kwargs):
        super(SaltbridgeLoader,self).__init__(db=db,**kwargs)

    def has_observable(self,trajectoryname):
        return self.db.has_saltbridge(trajectoryname)

    def read_datafile_iter(self,filename):
        """Return an iterator over records from the saltbridge file."""

        datafile = bz2.BZ2File(filename,'r')
        frame = 0   # count frames (1-based)
        showed_warning = {'toomany':False, 'toofew':False}
        try:
            for line in self.datalines(datafile):
                frame += 1
                values = map(float,line.split())
                time_ps = values.pop(0)  # ignored
                nval = len(values)
                ncol = len(self.columns)
                if nval < ncol:
                    values += (ncol - nval) * [None]
                    if not showed_warning['toofew']:
                        print "\tWarning: too few columns (%(nval)d) in input, "\
                              "should be %(ncol)d. Padding with None."  % vars()
                        showed_warning['toofew'] = True
                elif nval > ncol:
                    values = values[:ncol]
                    if not showed_warning['toomany']:
                        print "\tWarning: too many columns (%(nval)d) in input, "\
                              "database only understands %(ncol)d. REMOVING extra columns!" % vars()
                        showed_warning['toomany'] = True
                yield tuple(values + [frame])
        finally:
            datafile.close()


class SaltbridgeEnergyLoader(SaltbridgeLoader):
    """Insert saltbridge energy data into db.

    File format (white space separated); ignore time (just count lines to get
    the frame number) and match on name:

      # comment
      time/ps  ESB1 ESB2 ... ESB10


    # SALTBRIDGE INTERACTION ENERGY ANALYSIS
    # WRITTEN BY $ID: INTER_SB.INP 3138 2009-03-23 03:11:58Z OLIVER $
    # ALL ENERGIES IN KCAL/MOL; ENERGY IN 1AKE ("closed_mini1.crd") SUBTRACTED:
    #========================================================================
    # 1:D33-R156  2:K97-N190  3:D54-K157  4:D54-R156  5:D54-R167
    # 6:R36-D158  7:R36-E170  8:K57-D158  9:K57-E170 10:D118-K136
    #========================================================================
    # REFERENCE VALUES:
    # > -111.569  -75.4382  -85.251  -68.7127  -70.0139
    # > -146.923  -38.8597  -23.765  -96.5058  -131.52
    #========================================================================
    # TIME/PS  1    2    3    4    5    6    7    8    9    10
    1 89.4565 -17.7879 80.4458 63.6047 57.3684 113.736 5.6439 16.1713 85.3757 -26.512
    2 ....       
    """
    
    columns = ('ESB1','ESB2','ESB3','ESB4','ESB5','ESB6','ESB7','ESB8','ESB9','ESB10',
               'ESB11','ESB12','ESB13','ESB14')
    filename_patterns = (
        # standard pattern: all use that because this is what make_saltbridges.py spits out
        re.compile(r'(?P<trjname>.+?)(_pmf)?_inter_sb\.dat(\.bz2)?'),
        )
    
    def has_observable(self,trajectoryname):
        return self.db.has_saltbridge_energy(trajectoryname)


class RMSDLoader(ObservableLoader):
    """Insert RMSD data into db.

    File format: pickled numpy array (actually, it has one dimension too many
    so we used array[0])

       cRMSD               oRMSD                DeltaRMSD
       RMSD(X(t), 1AKE)    RMSD(X(t), 4AKE)     cRMSD - oRMSD
    """
    
    columns = ('cRMSD', 'oRMSD', 'DeltaRMSD')
    filename_patterns = (
        # standard pattern: all use that because this is what make_rms.py spits out
        re.compile(r'(?P<trjname>.+?)(_pmf)?_deltaRMSD\.pickle'),
        )
    
    def __init__(self,db=None,**kwargs):
        super(RMSDLoader,self).__init__(db=db,**kwargs)

    def has_observable(self,trajectoryname):
        return self.db.has_RMSD(trajectoryname)

    def read_datafile_iter(self,filename):
        """Return an iterator over records from the RMSD file."""

        import numpy
        data = numpy.load(filename)[0]  # NOTE: array was pickled with 3 dimens
        
        for n,(cRMSD,oRMSD,DeltaRMSD) in enumerate(data):
            yield (cRMSD,oRMSD,DeltaRMSD, n+1)

class ContactsLoader(ObservableLoader):
    """Insert Contacts data (q1-q2) into db.

    File format (white space separated); ignore time (just count lines to get
    the frame number) and match on name:

      # comment
      frame  q1 q2  n1 n2
    """
    
    columns = ('q1', 'q2', 'n1', 'n2')
    filename_patterns = (
        # (modelled after energy_patterns)
        # standard pattern: all use that because this is what make_fret.py spits out
        re.compile(r'(?P<trjname>.+?)(_pmf)?_q1q2\.dat(\.bz2)?'),
        )    
    
    def __init__(self,db=None,**kwargs):
        super(ContactsLoader,self).__init__(db=db,**kwargs)

    def has_observable(self,trajectoryname):
        return self.db.has_contacts(trajectoryname)

    def read_datafile_iter(self,filename):
        """Return an iterator over records from the FRET file."""

        datafile = bz2.BZ2File(filename,'r')
        try:
            for line in self.datalines(datafile):
                frame,q1,q2,n1,n2 = line.split()
                yield (float(q1), float(q2), int(n1), int(n2), int(frame))
        finally:
            datafile.close()


def setup(db_name=None,pmfonly=False,simulations=None,tempdir=None):
    """Setup and populate the default database.

    db = setup(NAME,pmfonly=True|False)

    This is a quick function to gather data from angle pickle files
    and set up the default database.

    NAME         name of the database; for performance reasons, do not write this to a
                 disk that is on a RAID 5 array (see eg
                 http://www.miracleas.com/BAARF/RAID5_versus_RAID10.txt)
                 NAME can be left empty for the default.
                 ':memory:' is also possible for a ultrafast db that only resides in memory
                 and is NEVER saved to disk.
    pmfonly      True: when building the database from scratch only select filenames that
                 contain *_pmf_* in the filename (== simulations=['pmf'])
                 False: collect data from ALL angle pickle files.
                        (== simulations=['pmf','dims','equilibrium'])
    simulations  list containing one or more of ['pmf', 'dims', 'equilibrium']
    tempdir      directory for temporary database files; see doc of AngleDB [None]

    Building a big database can take hours on a slow disk (especially on a RAID
    5 array) because of write performance issues on those system. Alternatively,
    one can build the db in memory (:memory:) and then clone it to disk, using

       db.clone(filename)

    which should be significantly faster. Then one can later reuse the db from
    disk. However, note that the clone database ONLY CONTAINS THE STATUS OF THE
    in-memory db RIGHT BEFORE CLONING. ANY FURTHER CHANGES TO THE in-memory db
    ARE LOST WHEN THE python SESSION IS ENDED.
    
    """
    import config

    # This function gathers a lot of the implicit knowledge about our file system layout
    # and what the trajectories actually are; not good but much better than nothing.
    # Note: keys in descriptions/filepatterns are lowercase
    descriptions = {'pmf': "pmf/umbrella sampled windows",
                    'dims': "co/oc DIMS runs",
                    'equilibrium': "equilibrium trajectories started from pdb structures",
                    }
    angles_dir = os.path.join(config.basedir,'data','new_angles')
    # path patterns are located under angles_dir
    filepatterns = {'pmf':   [('pmf','*_pmf_angles.pickle'),
                              ],
                    'dims':  [('DIMS','apo','standard','co*_angles.pickle'),
                              ('DIMS','apo','standard','oc*_angles.pickle'),
                              ],
                    'equilibrium': [('equilibrium','[0-9]???_*_angles.pickle'),
                                    ],
                    }

    if simulations is None:
        simulations = ['pmf','dims','equilibrium']
    # compatibility hack
    if pmfonly is True:
        simulations = ['pmf']

    # sanity check
    simulations = [x.lower() for x in simulations]
    for sim in simulations:
        if sim not in descriptions:
            raise ValueError("Sorry, simulation = '%(sim)s' is not " % vars() +
                             "recognized, only "+str(descriptions.keys()) )
        
    name_default = "angles_" + "_".join(simulations) + ".db"
    if db_name is None:
        db_name = name_default
    db_filename = db_name
    db = AngleDB(db_filename,tempdir=tempdir)

    if db.numrows < 200000:
        print "setup(): Populating database '%(db_filename)s'... can take a h'while... ;-)" % vars()

        def insertmany(sim):
            print "setup(): Loading '%s': %s ..." % (sim, descriptions[sim])
            for p in filepatterns[sim]:
                db.insertmany(os.path.join(angles_dir,*p))
        
        for sim in simulations:
            insertmany(sim)
        db.add_metadata()
    else:
        print "Database already contains %r frames, probably already setup." % db.numrows
    print "setup(): Database '%(db_filename)s' is ready." % vars()
    return db


def load_observables(db,pmf=True):
    """Load all observables.

    load_observables(db,pmf=True|False)

    pmf       True: load observables computed from pmf data
              False: load DIMS data
    """
    
    if pmf:
        mode = 'pmf'
    else:
        mode = 'DIMS'
    def datadir(*args):
        return os.path.join(config.basedir,'data',*args)


    # Hard coded directories where to find data; DIMS/PMF is not
    # always separate and we rely on the fact that we have no naming
    # clashes...

    # observable names must be the same as in AnglesDB.loaders
    datadirs = {'pmf': {'energy': datadir('energy','pmf','*.dat.bz2'),
                        'RMSD': datadir('rmsd','*.pickle'),
                        'FRET': datadir('fret','pmf','*.dat.bz2'),
                        'saltbridge': datadir('salt_bridge','pmf','*.dat.bz2'),
                        'saltbridge_energy': datadir('inter_sb','pmf','*.dat.bz2'),
                        'contacts': datadir('contacts', 'pmf', '*.dat.bz2'),
                        },
                'DIMS': {'energy': datadir('energy','dims','*.dat.bz2'),
                         'RMSD': datadir('rmsd','*.pickle'),
                         'FRET': datadir('fret','dims','apo','*.dat.bz2'),
                         'saltbridge': datadir('salt_bridge','dims','apo','*.dat.bz2'),
                         'saltbridge_energy': datadir('inter_sb','[co][oc]','*.dat.bz2'),
                         'contacts': datadir('contacts', 'dims', 'apo', '*.dat.bz2'),
                         },
                }

    print "-- selected mode = %(mode)s" % vars()
    D = datadirs[mode]
    for o,d in D.items():
        print "-- loading %(o)r from %(d)r" % vars()
        db.loaders[o].insertmany(d)   # choose loader based on observable type


def plot_windows_together(db,figname=os.path.join(config.basedir,'figs','pmf','windows.pdf'),stride=10,**plotargs):
    """Plot windows in one plot; stride selects a subset of windows.

    Example:

    >>> PMF.angles.plot_windows_together(db,figname='figs/pmf/all_windows.png',stride=1,alpha=0.3,contour_alpha=0.2,cmap=cm.jet_r)
    """
    import pylab

    pylab.clf()
    
    fn = db.filenames()[::stride]
    for n,f in enumerate(fn):
        fb = os.path.basename(f)
        print "-- %5.1f%% %3d/%3d %s" % (100*(n+1)/len(fn), n, len(fn), fb)
        # figname=os.path.join('figs','pmf',fb+'.pdf'),
        s = Selection(db,'filename="%s"' % f)
        s.plot(clf=False,**plotargs)
    pylab.title('Umbrella windows')
    pylab.savefig(str(figname))
    print "- Created figure '%(figname)s'." % vars()

def plot_windows(db,figdir=os.path.join(config.basedir,'figs','pmf','windows'),pattern="*",
                 stride=1,**plotargs):
    """Plot individual windows, one per plot + ref value; stride selects a subset of windows.

    Example:

    >>> PMF.angles.plot_windows(db,figdir='figs/pmf/windows',pattern='*1dvr*',
                                stride=1,alpha=1,contour_alpha=1,cmap=cm.hot_r)


    Arguments:

    figdir            directory where to dump figures
    pattern           glob pattern to match filenames against, eg '*1dvr*' [*]
    stride            only plot every stride-th entry (to get an overview) [1]
    **plotargs        keyword arguments that are used for Selection.plot()
    
    """
    import pylab
    import errno
    try:
        os.makedirs(figdir)
    except OSError,err:
        if err.errno != errno.EEXIST:
            raise

    plotargs.setdefault('mode','reldev')      # plot coverage as relative deviation from mean
    plotargs.setdefault('interval_contour',0.5)
    plotargs.setdefault('bins_LID', 50)

    was_interactive = pylab.matplotlib.is_interactive()
    pylab.matplotlib.interactive(False)

    meta = db.selection('SELECT * FROM __meta__')

    # signify force constant with marker
    forceconstants = numpy.unique(meta.recarray.forceconstant)
    markers = list("o^sDphv<>1234H")
    nmissing = len(markers) - len(forceconstants)
    if nmissing > 0:
        markers.extend( [markers[-1]] * nmissing )
    fcmarker = dict(zip(forceconstants,markers))

    NMPlimits,LIDlimits = db.angleranges()
    fn = db.filenames(pattern=pattern)[::stride]
    for n,f in enumerate(fn):
        fb = os.path.basename(f)
        print "-- %5.1f%% %3d/%3d %s" % (100.0*(n+1)/len(fn), n, len(fn), fb)
        figname=os.path.join(figdir,fb+'_coverage.png')
        s = Selection(db,'filename="%s"' % f)
        s.plot(**plotargs)
        r = meta.SQL('SELECT NMP_ref,LID_ref,forceconstant FROM __self__ WHERE filename = ?',f,asrecarray=True)
        if len(r) >  0:
	    print r
            #print "--- center NMP=%g  LID=%g    k=%g" % (r.NMP_ref[0],r.LID_ref[0],r.forceconstant[0])
            pylab.plot(r.NMP_ref, r.LID_ref,color='w',marker=fcmarker[r.forceconstant[0]],ms=8,mew=1,mec='r')
            pylab.xlim(NMPlimits)
            pylab.ylim(LIDlimits)
        else:            
            print "WW warning: no meta data for filename '%(fb)s'. " % vars()
        pylab.title(fb)            
        pylab.savefig(str(figname))
        print "--- Created figure '%(figname)s'." % vars()

    pylab.matplotlib.interactive(was_interactive)


def plot_coverage(db,use_blacklist=True):
    """Plot the total covrage of the unbiased histogram.

    >>> db = setup(pmfonly=True)
    >>> db.add_metadata()
    >>> plot_coverage(db)

    Simple hard-coded plotting routine. Adds two dots for the end
    points and focuses on the interesting region.

    db              pmfonly db
    use_blacklist   True: filter all files that appear in the blacklist [default]
    """
    from pylab import clf,plot,xlim,ylim,title
    if use_blacklist:
        print "Excluding anything listed in the blacklist (i.e. restricting to __meta__)"
        selection = db.selection("SELECT * FROM __data__")
    else:
        selection = db
    selection.plot(mode="reldev")
    #title(r'Umbrella sampling coverage: ${N}/{\langle{N}\rangle} - 1$')
    make_canonical_plot()


def make_canonical_plot(NMP_lim=(39,76),LID_lim=(99,154),
                        c1AKE=config.angles['1AKE'],
                        c4AKE=config.angles['4AKE'],
                        xray=True):
    """Scale current figure to default limits and plot the positions of 1AKE and 4AKE.

    The points for the end states are taken from txt/x-ray_angles.txt.

    If xray=True then add locations of the X-ray structures; this is
    the same as running plot_xary_structures().
    """
    import pylab
    if xray:
        plot_xray_structures()
    pylab.plot([c1AKE[0],c4AKE[0]], [c1AKE[1],c4AKE[1]], 'sw', ms=12, alpha=0.8)
    pylab.xlim(NMP_lim)
    pylab.ylim(LID_lim)

def plot_xray_structures():
    """Plot all x-ray structures on top of the current graph."""
    import pylab
    xrays = numpy.array(config.angles.values())  # (NMP,LID)
    pylab.plot(xrays[:,0],xrays[:,1], 'ow', ms=10, alpha=0.8)

def plot_transition(db,trajectory,**plotargs):
    """Plot one or more trajectories in angle space.

      lines = plot_transition(db,trajectory,**plotargs)

    The trajectory argument can be a glob pattern (but right now
    different trajectories are not separated). **plotargs are
    arguments for pylab plot, eg

        plot_transition(DIMSdb,'oc020', color='red', linewidth2)


    Use a database with transitions such as 

       DIMSdb = PMF.angles.setup(':memory:',simulations=['dims'])
    """
    import pylab
    plotargs['scalex'] = plotargs['scaley'] = False
    plotargs.setdefault('color','white')
    plotargs.setdefault('linestyle','-')        
    
    trj = db.SQL("SELECT NMP,LID FROM __self__ WHERE GLOB(?,trajectory)",
                 trajectory,asrecarray=True)
    if len(trj) == 0:
        raise ValueError('Found no trajectory data')
    if not hasattr(db,'_lines'):
        # extend db object on the fly...
        db._lines = {}   # store graphic objects here
        db._lines_annotation = {}
        # not working below ?!
        def _plot_transition(self,trajectory):
            return plot_transition(self,trajectory)
        def _rm_transition(self):
            return remove_last_transition(self)
        db.plot_transition = _plot_transition
        db.plot_transition.__doc__ = "plot_transition(trajectoryname)"
        db.rm_transition = _rm_transition        
    if not 'transitions' in db._lines:
        db._lines['transitions'] = []            # append lines
        db._lines_annotation['transitions'] = [] # append trajectory
    lines = pylab.plot(trj.NMP, trj.LID,'-',**plotargs)
    db._lines['transitions'].extend(lines)
    db._lines_annotation['transitions'].append(trajectory)
    print "Added trajectory path %(trajectory)r." % vars()    
    return lines

def remove_last_transition(db):
    import pylab
    l = db._lines['transitions'].pop()
    l.remove()    
    trajectory = db._lines_annotation['transitions'].pop()
    pylab.draw()
    print "Removed trajectory path %(trajectory)r." % vars()
    

def plot_all_transitions(db,**plotargs):
    """Plot all transitions in the db.

    lines = plot_all_transitions(db,**plotargs)

    E.g. use for the equilibrium transitions:

      EQ = PMF.angles.setup(':memory:',simulations=['equilibrium'])
      lines = plot_all_transitions(EQ,alpha=0.5)

    """
    import util
    cr = util.ColorRing()
    for t in db.trajectories:    # need __meta__
        plotargs['color'] = cr.get()
        plot_transition(db,t,**plotargs)

# quick copy and paste hacks...

